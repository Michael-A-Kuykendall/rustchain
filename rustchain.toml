[llm]
default_backend = "ollama"
ollama_base_url = "http://localhost:11434"
ollama_model = "tinyllama"
timeout_seconds = 30

[tools]
enabled = ["echo", "math"]

[logging]
level = "info"
